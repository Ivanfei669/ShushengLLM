## Xtuner 微调

### Fine Tune
#### 1.1 两种微调范式
##### 增量预训练
使用场景：让基座模型学习到一些新知识

训练数据：文章、书籍、代码等

##### 指令跟随微调
使用场景：让模型学会对话模版，根据人类指令进行对话

训练数据：高质量的对话、问答数据

#### 1.2 LORA& QLoRA
在基座模型旁，新增一个支路（adapter）， adapter参数量远小于基座模型，能大幅降低训练的显存消耗。

基座模型量化时，用4-bit量化

### Xtuner

### InternLM2 1.8B 
