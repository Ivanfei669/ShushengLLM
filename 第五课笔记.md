## LMDeploy 量化部署 LLM-VLM 实践
### 模型部署
把训练好的模型放到特定环境中。
#### 1.1 大模型部署难点
##### 计算量巨大
$$
C_{forward} = 2N + 2n_{layer}n_{ctx}d_{attn}
$$

#### 内存开销巨大
$$
M_{kvcache} = 4bn_{layer}d_{attn}(s + n)
$$

#### 访存瓶颈
硬件计算速度远快于显存带宽

#### 1.2模型部署方法
##### 模型剪枝（Pruning）
移除模型中不必要或多余的组件。在保证性能最低下降的同时，可以减小存储需求、提高计算效率。

##### 知识蒸馏
通过轻量化的学生模型模仿性能更好、结构更复杂的教师模型。

##### 量化
浮点数转换为整数或其他离散形式，从而减轻模型的存储和计算负担。（减少访存量）

### LMDeploy
涵盖LLM全套轻量化部署和服务解决方案。

#### 2.1 模型高效推理 lmdeploy chat -h

#### 2.2 模型量化压缩 lmdeploy lite -h

#### 2.3 服务化部署 lmdeploy serve -h


